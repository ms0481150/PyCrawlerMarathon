{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item.py\n",
    "import scrapy\n",
    "\n",
    "class PttcrawlerItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "\n",
    "    url = scrapy.Field()\n",
    "    article_id = scrapy.Field()\n",
    "    article_author = scrapy.Field()\n",
    "    article_title = scrapy.Field()\n",
    "    article_date = scrapy.Field()\n",
    "    article_content = scrapy.Field()\n",
    "    ip = scrapy.Field()\n",
    "    message_count = scrapy.Field()\n",
    "    messages = scrapy.Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings.py\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "ITEM_PIPELINES = {\n",
    "#    'PTTcrawler.pipelines.PttcrawlerPipeline': 300,\n",
    "    'PTTcrawler.pipelines.JSONPipeline':10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "class PttcrawlerPipeline(object):\n",
    "    def process_item(self, item, spider):\n",
    "        return item\n",
    "\n",
    "\n",
    "class JSONPipeline(object):\n",
    "    def open_spider(self, spider):\n",
    "        self.start_crawl_datetime = datetime.now().strftime('%Y%m%dT%H:%M:%S')\n",
    "\n",
    "        # 在開始爬蟲的時候建立暫時的 JSON 檔案\n",
    "        # 避免有多筆爬蟲結果的時候，途中發生錯誤導致程式停止會遺失所有檔案\n",
    "        self.dir_path = Path(__file__).resolve().parents[1] / 'crawled_data'\n",
    "        self.runtime_file_path = str(self.dir_path / '.tmp.json.swp')\n",
    "        if not self.dir_path.exists():\n",
    "            self.dir_path.mkdir(parents=True)\n",
    "        spider.log('Create temp file for store JSON - {}'.format(self.runtime_file_path))\n",
    "\n",
    "        # 設計 JSON 存的格式為\n",
    "        # [\n",
    "        #  {...}, # 一筆爬蟲結果\n",
    "        #  {...}, ...\n",
    "        # ]\n",
    "        self.runtime_file = open(self.runtime_file_path, 'w+', encoding='utf8')\n",
    "        self.runtime_file.write('[\\n')\n",
    "        self._first_item = True\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        # 把資料轉成字典格式並寫入文件中\n",
    "        if not isinstance(item, dict):\n",
    "            item = dict(item)\n",
    "\n",
    "        if self._first_item:\n",
    "            self._first_item = False\n",
    "        else:\n",
    "            self.runtime_file.write(',\\n')\n",
    "\n",
    "        self.runtime_file.write(json.dumps(item, ensure_ascii=False))\n",
    "        return item\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.end_crawl_datetime = datetime.now().strftime('%Y%m%dT%H:%M:%S')\n",
    "\n",
    "        # 儲存 JSON 格式\n",
    "        self.runtime_file.write('\\n]')\n",
    "        self.runtime_file.close()\n",
    "        \n",
    "        # 將暫存檔改為以日期為檔名的格式\n",
    "        self.store_file_path = self.dir_path / '{}-{}.json'.format(self.start_crawl_datetime,\n",
    "                                                                   self.end_crawl_datetime)\n",
    "        self.store_file_path = str(self.store_file_path)\n",
    "        os.rename(self.runtime_file_path, self.store_file_path)\n",
    "        spider.log('Save result at {}'.format(self.store_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PTT.py\n",
    "\n",
    "import scrapy\n",
    "from PTTcrawler.items import PttcrawlerItem\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "\n",
    "class PttSpider(scrapy.Spider):\n",
    "    name = 'PTT'\n",
    "    allowed_domains = ['www.ptt.cc']\n",
    "    start_urls = ['https://www.ptt.cc/bbs/Examination/M.1641347124.A.630.html']\n",
    "    cookies = {\"over18\": \"1\"}\n",
    "\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse, cookies=self.cookies)\n",
    "\n",
    "    def parse(self, response):\n",
    "        # 檢查網頁回應是否正確\n",
    "        if response.status != 200:\n",
    "            print('Error - {} is not available to access'.format(response.url))\n",
    "            return\n",
    "        \n",
    "        # response 傳入 bs\n",
    "        soup = BeautifulSoup(response.text)\n",
    "\n",
    "        # 取得文章內容主體\n",
    "        # 標題、作者、內容div id皆為main-content\n",
    "        main_content = soup.find(id='main-content')\n",
    "\n",
    "        # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "        metas = main_content.select('div.article-metaline')\n",
    "        author = ''\n",
    "        title = ''\n",
    "        date = ''\n",
    "        if metas:\n",
    "            if metas[0].select('span.article-meta-value')[0]:\n",
    "                author = metas[0].select('span.article-meta-value')[0].string\n",
    "            if metas[1].select('span.article-meta-value')[0]:\n",
    "                title = metas[1].select('span.article-meta-value')[0].string\n",
    "            if metas[2].select('span.article-meta-value')[0]:\n",
    "                date = metas[2].select('span.article-meta-value')[0].string\n",
    "\n",
    "            # 提取完作者、標題、日期後，移除掉\n",
    "            # .extract() 方法可以參考官方文件\n",
    "            #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "            for m in metas:\n",
    "                m.extract()\n",
    "            for m in main_content.select('div.article-metaline-right'):\n",
    "                m.extract()\n",
    "\n",
    "        # 取得留言區主體\n",
    "        pushes = main_content.find_all('div', class_='push')\n",
    "        for p in pushes:\n",
    "            p.extract()\n",
    "\n",
    "        # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "        # 透過 regular expression 取得 IP\n",
    "        # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "        try:\n",
    "            ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "            ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "        except Exception as e:\n",
    "            ip = ''\n",
    "\n",
    "        # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "        # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "        #\n",
    "        # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "        #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "        filtered = []\n",
    "        for v in main_content.stripped_strings:\n",
    "            # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "            if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "                filtered.append(v)\n",
    "\n",
    "        \n",
    "        # 定義一些特殊符號與全形符號的過濾器\n",
    "        expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "        for i in range(len(filtered)):\n",
    "            filtered[i] = re.sub(expr, '', filtered[i])\n",
    "\n",
    "        # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "        filtered = [i for i in filtered if i]\n",
    "        content = ' '.join(filtered)\n",
    "\n",
    "\n",
    "        # 處理留言區\n",
    "        # p 計算推文數量\n",
    "        # b 計算噓文數量\n",
    "        # n 計算箭頭數量\n",
    "        p, b, n = 0, 0, 0\n",
    "        messages = []\n",
    "        for push in pushes:\n",
    "            # 假如留言段落沒有 push-tag 就跳過\n",
    "            if not push.find('span', 'push-tag'):\n",
    "                continue\n",
    "\n",
    "            # 過濾額外空白與換行符號\n",
    "            # push_tag 判斷是推文, 箭頭還是噓文\n",
    "            # push_userid 判斷留言的人是誰\n",
    "            # push_content 判斷留言內容\n",
    "            # push_ipdatetime 判斷留言日期時間\n",
    "            push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "            push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "            push_content = push.find('span', 'push-content').strings\n",
    "            push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "            push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "            # 整理打包留言的資訊, 並統計推噓文數量\n",
    "            messages.append({\n",
    "                'push_tag': push_tag,\n",
    "                'push_userid': push_userid,\n",
    "                'push_content': push_content,\n",
    "                'push_ipdatetime': push_ipdatetime})\n",
    "            if push_tag == u'推':\n",
    "                p += 1\n",
    "            elif push_tag == u'噓':\n",
    "                b += 1\n",
    "            else:\n",
    "                n += 1\n",
    "\n",
    "        # 統計推噓文\n",
    "        # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "        # all 為總共留言數量 \n",
    "        message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "\n",
    "        # 整理文章資訊\n",
    "        data = PttcrawlerItem()\n",
    "        article_id = str(Path(urlparse(response.url).path).stem)\n",
    "        data['url'] = response.url\n",
    "        data['article_id'] = article_id\n",
    "        data['article_author'] = author\n",
    "        data['article_title'] = title\n",
    "        data['article_date'] = date\n",
    "        data['article_content'] = content\n",
    "        data['ip'] = ip\n",
    "        data['message_count'] = message_count\n",
    "        data['messages'] = messages\n",
    "        yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'dict'>\n",
      "https://www.ptt.cc/bbs/Examination/M.1641347124.A.630.html\n",
      "M.1641347124.A.630\n",
      "--------------------\n",
      "gary22204 ()\n",
      "[閒聊] 110高考資訊處理，如果我考第16名怎麼辦?\n",
      "Wed Jan  5 09:45:22 2022\n",
      "--------------------\n",
      "安安我上榜學長啦\n",
      "\n",
      "昨天中午跟公家好同事一起出去運動，想起是放榜日\n",
      "\n",
      "畢竟我們單位今年好像也有缺，就看了一下榜單\n",
      "\n",
      "不看得了，一看嚇死我的寶寶\n",
      "\n",
      "資訊處理高考居然只有15個正額錄取\n",
      "\n",
      "上榜學長我尋思，這年頭資訊爆炸，資訊缺只會愈來愈多，顯然不正常\n",
      "\n",
      "這時候...(柯南音樂下)...真相只有一個\n",
      "\n",
      "\n",
      "於是，看了最新的暫定需用名額統計表，居然有80個\n",
      "\n",
      "80葛AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "但4只有15個人入取AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "所以有兩個可能：1.考題太難大家都沒到50分    2.閱卷的人太雕\n",
      "\n",
      "真相果然只有一葛\n",
      "\n",
      "\n",
      "\n",
      "於是，學長我就想了一下\n",
      "\n",
      "如果我含辛茹苦全職讀了一整年兩整年\n",
      "\n",
      "好不容易考了全國前16名，準備谷底反轉，泥封高輝\n",
      "\n",
      "但4，好像落榜了AAAAAAAAAAAAA\n",
      "ob_ov\n",
      "\n",
      "\n",
      "又，資訊單位在這疫情這下死撐活撐2年，考試又延了快半年\n",
      "\n",
      "終於迎來新人補進的時機了\n",
      "\n",
      "結果，國家一整年的補進資訊人員的機會，居然因為出題過難\n",
      "\n",
      "就這樣嚴重拖慢國家的效率\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "身為103年的學長我只有一個感想\n",
      "\n",
      "拖慢考生拖慢機構，真不愧是我大鬼島台灣\n",
      "\n",
      "\n",
      "\n",
      "請教今年有考試的考生們，到底是閱卷太嚴苛\n",
      "\n",
      "還是人家所謂的年輕人不夠努力，一代不如一代?ob~~~ov\n",
      "\n",
      " https://www.ptt.cc/bbs/Examination/M.1641347124.A.630.html\n",
      "IP： 117.56.223.223\n",
      "總留言數:26\n",
      "推噓差:13\n",
      "推文數:14\n",
      "噓文數:1\n",
      "箭頭數:11\n",
      "--------------------\n",
      "推 carterdunk 天下無難事，只要有新人。 01/05 09:48\n",
      "→ carterdunk 但沒有新人怎麼辦？ 01/05 09:49\n",
      "→ vv956 標準在那邊，依法行政～ 01/05 09:49\n",
      "→ carterdunk 年輕人要好好努力，萬事在人為。 01/05 09:49\n",
      "推 esnothing 全到且遭否決率98.29% 01/05 09:51\n",
      "推 CGF16469 錄取率1.71、新死亡之組誕生 01/05 10:00\n",
      "推 Jerome16 出題閱卷老師用心良苦，要大家躲過舊制退休金末班車 01/05 10:09\n",
      "噓 sinksink 泥不用擔心那麼多，跟土木搞到一樣缺比人多，考選部那些 01/05 10:11\n",
      "→ sinksink 高級智障就會出來勸那些老鬼改高一點了… 01/05 10:12\n",
      "→ mathiloveyou 第16名不會知道自己是第16名,免擔心 01/05 10:12\n",
      "→ sinksink 樓上，那只是成績單上沒印，吃飽撐著看人數統計就知道了 01/05 10:14\n",
      "→ gary22204 級距算一下，還是能知道個7788啦 01/05 10:14\n",
      "推 lanx105 有人考7x欸好猛 01/05 10:18\n",
      "推 ihl123456 考那些真正會用到的 看有沒有1% 01/05 10:28\n",
      "推 killrice 總分未滿50分就不會有排名，要自己看統計表推排名。 01/05 11:13\n",
      "推 youaremybabe 但未滿50分的在統計表上只會被放在“全到且遭否決 01/05 11:42\n",
      "→ youaremybabe ”裡面，還是看不出來名次排到哪裡 01/05 11:42\n",
      "推 carterdunk 慘 01/05 11:59\n",
      "推 h14753951 16名以後的人會在國營資訊職員的榜單上被看到XD 01/05 12:34\n",
      "推 yoshiringo Orz 01/05 12:44\n",
      "推 chter 八成去考國營了吧 01/05 12:46\n",
      "→ maiico 沒上也好啦 01/05 13:27\n",
      "推 skizard 資訊類 業界大缺人時代，建議停損去業界吧 01/05 13:28\n",
      "推 jkl4566654 國營榜單出現+1 01/05 13:59\n",
      "→ wave1et 一堆補習班仔阿，非補習班教材就gg 01/05 14:05\n",
      "→ liushengzhe 怎辦？去領黨證，進外包部。 01/05 15:05\n"
     ]
    }
   ],
   "source": [
    "# 存好的JSON\n",
    "\n",
    "import json\n",
    "with open(\"./Data/PTT_crawler/crawled_data/.tmp.json.swp\", encoding=\"utf-8\") as j:\n",
    "    data = json.load(j)\n",
    "print(type(data))\n",
    "print(type(data[0]))\n",
    "\n",
    "print(data[0]['url'])\n",
    "print(data[0]['article_id'])\n",
    "print(\"--------------------\")\n",
    "print(data[0]['article_author'])\n",
    "print(data[0]['article_title'])\n",
    "print(data[0]['article_date'])\n",
    "print(\"--------------------\")\n",
    "print(data[0]['article_content'])\n",
    "print(\"IP：\", data[0]['ip'])\n",
    "print(\"總留言數:{}\".format(data[0]['message_count']['all']))\n",
    "print(\"推噓差:{}\".format(data[0]['message_count']['count']))\n",
    "print(\"推文數:{}\".format(data[0]['message_count']['push']))\n",
    "print(\"噓文數:{}\".format(data[0]['message_count']['boo']))\n",
    "print(\"箭頭數:{}\".format(data[0]['message_count']['neutral']))\n",
    "print(\"--------------------\")\n",
    "for i in data[0]['messages']:\n",
    "    print(i['push_tag'], i['push_userid'], i['push_content'], i['push_ipdatetime'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d15be550ff1de484891e07a12ec761e7964c50a27d22b0326217da8277522cd7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('crawler': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
